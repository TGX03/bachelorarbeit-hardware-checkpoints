\chapter{Approach \& Implementation}\label{chap:implementation}
As the aim of this thesis is to provide information and a solution in a general fashion,
meaning it works as independently from the architectures in use as possible,
the approach used should be outlined beforehand.
This chapter will fulfill this requirement
and explain the ideas for implementation,
and will also include dropped approaches with explanations of
why they were either suboptimal or downright impossible to realize.
To do this, this chapter will be sectioned into a general section explaining interaction with QEMU,
which affects any kind of data exchange,
as well as the approaches for extracting the different kinds of data,
as each one requires an individual solution.

\section{Development environment}
The main topic of this section is to explain the decision for a programming language and the related toolkit.
As this is supposed to be a generic solution, a programming language with a high abstraction level,
preferably running on some sort of runtime allowing for the direct execution of the executable on any kind of system, is preferred.
Examples of such platform-independent languages are Python, Java, Ruby or PHP.

As the operations may involve operations inside the memory of another process,
a language which is suited for this manipulation should be used.
Such a task also requires a lot of performance to process such large memory regions.
Especially the task of checking memory regions or even file systems for equal contents is very compute-heavy.
The default for such a task is C or Assembly, and C++, Go, and unsafe Rust are also capable of such tasks.

Also, such tasks produce a lot of side effects by definition,
making any kind of functional programming language a hindrance if used\todonote{Haskell-Trauma}.

\subsection{Java}
\enquote{Write once, run anywhere}\cite{WORA} was the motto under which Java was shipped originally,
and to this day it and its environment are one of the most well-known platform-independent
programming languages and runtimes.
In its now nearly 30 years of existence, it has received consistent updates to improve performance,
and though to this day, a valid performance evaluation of any given Java program remains challenging\cite{Java_Benchmarking},
the performance of Java seems to be within one order of magnitude when compared to C++ and 2 orders of magnitude when compared to plain C,
while however requiring a higher development effort and having a much larger memory footprint\cite{Java_Performance}.
Since Python is the only other JIT-based contender in \citetitle{Java_Performance} being somewhat faster,
however featuring even fewer capabilities to directly manipulate memory without
resorting to large external libraries, the decision was made to use Java for this thesis.

\label{JNA}
Especially the availability of tools like \emph{Java Native Access}\cite{JNA},
which allows for the ability to use native system libraries while staying inside Java code,
for which Python or other high-level programming languages feature no alternative,
reinforced the decision to use Java in this thesis.
Later on, it turned out such tools were not required, however when initially planning this thesis
the potential necessity to directly call functions offered by the operating system led to the decision to use Java.

The \emph{java.net} package is also one of the most capable and straightforward implementations of networking,
and since the communication with QEMU, as shown in \autoref{sec:QEMU_Interaction}, will rely heavily on TCP,
these capabilities also influenced the pro-Java decision.

\subsection{Checkpoint format}
The checkpoint format is an important piece for future work based on this thesis,
and therefore be explained here.
The basic format used is JSON, as it is effectively the standard for exchanging data
in an easily parseable way without causing trouble when a small part of the data format is changed
like it is the case with serialized data exchange formats.

The exact format will be explained in the section referring to the corresponding data point.
Here only the top-level container holding the different data points will be explained:
\begin{itemize}
    \item \emph{cpu} contains an array made up of CPU objects, each containing all register content and additional data of each CPU.
    \item \emph{memory} is an array containing all the memory regions extracted.
    The concept of memory regions is explained in \autoref{sec:memory_extraction}
    \item \emph{blockdevice} is an array containing all data of block devices,
    referring to any kind of drives mounted to an emulation instance.
    It also stores data about drives not containing any media, for example empty CD drives,
    but obviously does not store any contents of such devices.
\end{itemize}

\section{Interaction with QEMU}\label{sec:QEMU_Interaction}
As discussed in \autoref{chap:QEMU_API} the main interface QEMU offers to external machine connections is QMP.
A QMP connection to a QEMU instance can be established using two ways:
Either through a Unix socket or Telnet running on a traditional TCP socket.
The usage of a Unix socket seems to be the preferred way,
however since it obviously only works on a Unix system,
this thesis chose to use Telnet to make this thesis applicable
to Windows and other operating systems as well.

In Java, such a telnet connection can simply be set up by opening a socket connecting to the port the QEMU host is listening on
and then sending the commands as the bytes of the string representing the command.
As QEMU sends data through this channel both as direct replies to requests as well as asynchronously
in case an error occurs or a prolonged action finishes,
the receiver needs to constantly listen for any incoming data and be able to process them.
For this, a thread needs to be running as well as interfaces be created
which can react when data matching their task is received.

\section{CPU registers}
This section is rather trivial, as the contents of the registers can simply be queried
by first requesting details from QEMU about all existing CPUs by using \emph{query-cpus-fast}
and then applying the findings from \autoref{sec:info_registers}.
The current implementation however is rather rudimentary,
as the specialities of architectures do not get taken into consideration.
This means the contents of registers as well as flags written
\enquote{according to standard}\footnote{There is no standard. Standard herein refers to what I observed in my testing, which of course is highly individual and described in \autoref{sec:info_registers}.}
get saved, however any other data is lost.
This means anyone reading data from QEMU needs to be very familiar with the architecture in use
and read the related specification like for example the \Citetitle{intel-manual}.

An example of such an issue is the floating point unit in ARM,
which is disabled on boot and needs to be explicitly enabled by the operating system.
QEMU of course replicates this behavior,
and when querying the state of the CPU, it explicitly returns
\enquote{FPU disabled} respective \enquote{FPU enbabled}
as shown in \autoref{fig:registers_ARM} for the state of the FPU.
When such information is necessary, any user of this thesis must implement the parsing of such data themselves.

\subsection{Checkpoint format}
Each CPU object consists of the following fields:
\begin{itemize}
    \item \emph{id} referring to the number of the core.
    \item \emph{architecture} holds the architecture of this CPU core.
    QEMU outputs this for every CPU individually,
    though currently QEMU is not capable of running an emulation with different CPU architectures at the same time.
    \item \emph{hostId} is the ID of the host QEMU thread responsible for simulating this CPU.
    \item \emph{registers} holds all registers associated with this CPU.
    For each register, the following fields are stored:
    \begin{itemize}
        \item \emph{name} is the name of the register provided by QEMU.
        \item \emph{contents} holds the bits of each register formatted as hex.
        Gets stored as a string instead of a JSON int to keep leading zeroes.
        \item \emph{size} stores the size the program determined for a register.
        Keep in mind the restrictions laid out in \autoref{sec:info_registers}.
    \end{itemize}
    \item \emph{flags} exists only if flags were found.
    This is currently an unsafe state because of the inconsistencies described in \autoref{sec:info_registers},
    so therefore it is recommended to check whether it holds correct information for any used architecture.
\end{itemize}

\section{System memory}\label{sec:memory_extraction}
Extracting system memory proved to be the most difficult task of this research.
Not only because of QEMU's rather complicated memory model,
which was explained excellently by \Citeauthor{kitcheckpoints}\cite{kitcheckpoints},
but also because it is pretty evident QEMU's external APIs do not serve the purpose of finding data about the emulation process itself.
Because of this, many different approaches were tried to access this data
and many assumptions were proven wrong.
Those approaches as well as the final solutions are going to be explained in the following,
as well as issues still existing in the final implementation.

\subsection{Using JNA to access QEMU memory}\label{sec:JNA_failure}
As explained in \autoref{JNA} Java Native Access is a library to directly work with APIs and libraries provided by the operating system.
It also has bindings for the Win32-API directly implemented,
basically allowing native usage of these APIs.
One ability these APIs allow is direct access to another process' memory.
The idea now was to find the location where QEMU stores the guest's memory,
and directly extract the data from there while optionally comparing it to previously stored data
to allow for deduplication.

As already evident by the mention of the Win32-API,
the first attempts to access the internal memory of QEMU were made under Windows
and are therefore only valid on Windows.
If this approach would have yielded results,
an adaptation to Linux would have also been created
which may even have had the ability to be generalized into the Unix world.
However, as this ultimately could not be made to work,
such a transfer was not attempted.

It also needs to be mentioned that it was tried to avoid using QEMU's functions to perform a full dump to disk,
but instead to somehow gain access to QEMU memory and read the data from there.
This path was initially chosen to avoid creating massive memory dumps \emph{before} checking for equal contents,
to save on disk space and disk access time.
Instead, the idea was to first transfer the data into the memory of the extraction tool,
in which such checks for equality and perhaps further compression could have been realized
before performing an expensive write to disk.
The obvious issue here is, if one could even get it to work,
the massive requirement for system memory this could have,
as memory consumption for every running QEMU instance would have doubled
during the time of checkpoint creation.

The naive approach here was to just take the physical address returned by \enquote{info tlb} or \enquote{info mtree}
and read data from there in the hope these were the actual offsets used internally.
Especially the fact the physical addresses do not start at address 0
made this an idea at least worth testing
in hopes it either works immediately or gives further leads to track down the actual memory location.
However, this directly turned up an issue:

\paragraph{Address randomization}
All modern operating systems randomize addresses of loaded programs for increased security.
This means, there usually is no data when reading from address 0 of a process,
but instead an error gets returned by the call to \emph{ReadProcessMemory}.
Instead, one must first load the list of all modules of a process,
called HMODULE in the Win32-API, find the one with the correct name,
in this case always the first one, as the executable is always the first module,
and then access its base address.
Only now is any access to the memory of the process even possible,
as all accesses before were made to memory regions
not existing in the virtual memory space of the process.

After having gained the ability to actually read QEMU's memory from another process,
the physical addresses provided by \eqnuote{info tlb} were tested,
but it was quickly determined these were not the actual addresses in QEMU's memory.
This could be determined because QEMU already offers ways of dumping memory itself.
One of these is the QMP command \emph{dump-guest-memory},
which was already presented in \autoref{dump_memory}.
Two other options are the QHM commands \emph{memsave} and \emph{pmemsave} from \autoref{sec:memsave}.
These are far less capable, however they produce a raw dump without any metadata,
which makes quick testing much easier.
But, when trying for a full dump with additional metadata,
\emph{dump-guest-memory} from QMP is still preferred.

From this point on, the main topic of research was to find any hint for the location of memory objects
which could be accessed using external APIs.
\Citeauthor{kitcheckpoints} used internal QEMU functions to access the RAM list
and access the individual memory region objects,
from which all RAM data can be accessed\cite{kitcheckpoints}.
This is probably the most elegant solution possible at the moment,
as it allows direct access as desired by this thesis
while being very fast, as no access to the hard disk is required.
The big drawback are of course the necessary modifications to QEMU's code to make this work,
something which this thesis wants to avoid to maintain usability and functionality for any future QEMU versions.
For this reason, this approach was discarded as it could potentially break with any new QEMU version.
Especially when trying to work with a debugger to call these internal functions,
it may not survive the next update to QEMU.
In addition, it also prevents usage on an already existing QEMU instance
which is already running some form of emulation that should be parsed without modification.

\subsection{Searching for memory regions}
The next, still rather naive approach,
was to get the start of a memory dump and to search for the contents inside the QEMU process.
For this, using one of the commands mentioned before would create a small dump,
and the developed program would then search through the whole of QEMU's memory
to find a region with identical contents.
This approach has many rather obvious issues.

\paragraph{Performance} It's going to be very slow.
It must read through the whole memory of the qemu.exe module,
which isn't something the Win32 API was designed to handle.
Win32 was designed for very small reads, as such access is both a security risk
and requires traversal of multiple page tables.
Additionally, external scanning isn't possible,
meaning requesting Windows to search for memory contents itself isn't possible.

\paragraph{Duplicate memory}
When manually getting dumps and searching through process memory with a hex editor,
even when using a sample size of 1024 bytes multiple matching regions were found.
In addition, uninitialized memory, which is made up of all zeroes,
makes this basically impossible,
as an uninitialized memory region already mapped to the guest would be impossible to correctly categorize.

\paragraph{Amount of memory objects}
A xemu\cite{xemu} instance, which has 2GB of RAM allocated, uses, depending on circumstances,
about 50 memory objects, each with its own physical and virtual memory region,
and also the ARM example in \autoref{fig:mem_ARM_full} with 2GB of main memory features 58 memory regions if not counting duplicates.
An emulation instance with more memory allocated,
or complicated mappings between host RAM, disk-backed memory and virtual devices could potentially have many more.
In combination with the previous issue of duplicate or uninitialized memory, this is basically impossible.

This approach was experimented with for a short amount of time,
but after encountering the first instances of memory with equal content,
it was effectively dropped and no longer pursued
as it was determined this could never be made reliable to the required extent
while putting any potential performance improvement of such a process into question.

\subsection{File backed memory}
As mentioned in \autoref{sec:API_mtree},
it is possible to create a region of memory inside a QEMU instance
which is directly mapped to disk memory.
When creating a file large enough to hold the entire memory of a QEMU instance,
one could read this file when the emulation is paused and store the checkpoint from this data.
This way was tried regularly, one example is from \Citeauthor{QEMU_memory_file},
who wrote a tutorial on how to do this\cite{QEMU_memory_file}.
Still, there are certain issues this solution creates,
and while none of these renders this approach impossible,
they need to be taken into account.

\subsubsection{Performance}
There are reasons why RAM exists and not all operations are simply performed directly on disk.
These reasons are latency and memory bandwidth.
Modern DDR5 RAM is specified for at least 3200MT/s (Megatransfers per second\footnote{A transfer refers to a single write or read operation. One megatransfer therefore consists of a million read or write operations per second.}),
with usual applications being between 4800MT/s and 5600MT/s,
and the specification allowing for up to 8800MT/s,
which results in a specified data rate between 12.8GB/s and 33.6GB/s per available channel.
At the same time, the latency of DDR5 is specified at around 13-18ns,
depending on the speed and quality of the chips chosen,
with overclocked memory being available allowing for even lower latencies\cite[p.392-406]{JEDEC}.

A typical NVMe hard drive uses 4 PCIe lanes, which on PCIe 5.0 equates to a maximum speed of 15.7GB/s,
while usually having latencies of around 2ms.\todonote{I need a source here that isn't just the datasheet of some random drive}
This means the most modern NVMe drives may be able to keep up with RAM in terms of data throughput,
but in terms of latency they are about 100 times worse,
which would make their usage as system memory a rather troublesome experience.
In addition, the current standard for NVMe is still PCIe 4.0, which makes this even worse.

\Citeauthor{QEMU_memory_file} provides a solution for this,
which consists of simply putting the file in a file system allocated in host RAM.
The solution he proposes for this on Linux is rather simple as Linux offers support for this natively.
However, the experience on other operating systems may vary, under Windows one would need additional software for this,
making this not the desired platform-independent solution.

\subsubsection{Flexibility}
This only works if the whole guest's memory can be mounted to a large memory file.
However, as shown in \autoref{fig:mem_ARM},
there are instances of QEMU needing to have specific memory regions mapped in specific ways.
If the guest memory must only be mounted to specific files,
it is of course possible to just read these files as well.
When emulating external devices however,
this is not applicable, as that memory must always be mounted by QEMU itself.

In addition, this means to use the findings of this thesis,
the targeted QEMU instance must again be prepared for the extraction process,
which is something this thesis tries to avoid.

For these reasons, it was also decided not to follow this approach if possible.

\subsection{Letting QEMU dump to disk}\label{sec:resignation}
After trying the previous approaches,
I was not aware of any other way to find the addresses of QEMU's memory regions
without injecting additional code into QEMU,
either directly by modifying the codebase or indirectly by attaching a debugger.\todonote{Maybe write this in a way people don't think I was too lazy to do this.}
This means the APIs offered by QEMU must be used directly,
with the potential performance penalty attached to it,
which will later be evaluated in \autoref{chap:Evaluation}.\todonote{Make this reference more precise once the chapter is written.}

QMP only offers a single command which extracts data from a running QEMU instance,
namely \emph{dump-guest-memory}.
It allows for dumping of guest memory to either ELF, Windows crashdump or raw formats,
depending on the guest's architecture.
In addition, it allows for paging, meaning to include both virtual and physical addresses in the dumped file.
This however is currently only possible for x86 guests and when using the ELF format for the dumped file.

\subsubsection{ELF}
Since ELF is a common standard, parsing an ELF file by itself is no special procedure.
It is however rather uncommon in Java, as Java programs don't get shipped as ELFs
and Java stacktraces are not comparable to a dump produced by a program crash.
There is one Java library called \citetitle{jelf} which offers the parsing of ELF files.
It seems to be developed by a single developer called \Citeauthor{jelf},
and while he does regularly work on this library,
its stability is still up to question and will be inspected further in \autoref{chap:Evaluation}\cite{jelf}.\todonote{Make this reference more precise once the chapter is written}

Also it appears the library was not explicitly developed for processing dumps
and even extracting data from them,
as the parts of the library for this functionality are very much undocumented.
Still, a process to read the data from such a dump could be established and now looks as follows:
\begin{enumerate}
    \item Read the file from an input stream.
    The option to read from a byte array is also offered,
    however this option falls flat for large files because of the reasons mentioned in \autoref{sec:Java_2GB}.
    \item Parse all program headers. Jelf only parses them when explicitly requested,
    which means this process cannot be run in parallel as this corrupts the internal state of the used input stream.
    \item The program headers required are of type \emph{PT\_LOAD},
    meaning a segment which can simply be loaded into memory.
    It is assumed all program headers of this type refer to a memory area of the guest,
    as there seems to be no other data in the ELF file allowing for such differentiation.
    \item Each header of type \emph{PT\_LOAD} carries the fields
    \emph{offset}, \emph{filesz}, \emph{paddr} and \emph{vaddr}.
    The first two describe where the data is in the file,
    the latter two describe where the data is from the perspective of the guest operating system.
    \item \emph{offset} and \emph{filesz} get used to extract this memory region from the ELF file into memory.
    \item If a previous checkpoint is linked, it is now checked
    whether the previous checkpoint has a memory region with the same contents available,
    and if so, the previous one gets linked instead.
    \item If no duplicate was found,
    the segment gets written to a file at the specified location,
    with its physical address getting used as the filename.
\end{enumerate}

\paragraph{Program header fields}
As mentioned, a program header in an ELF file has 4 fields which are relevant to this thesis.
The first is \emph{offset}, which simply references the first byte of this area inside the file.
\emph{filesz} shows the number of bytes stored for this region in the ELF file.
This may differ from \emph{memsz}, which is the actual size of the memory region when loaded inside QEMU.
I have never seen these two values differ, therefore only \emph{filesz} is read,
however this may need changing if discrepancies are found.
\emph{paddr} is the physical address of the first byte of this segment in the QEMU memory model,
while \emph{vaddr} is the first virtual address the guest operating system sees for this memory region.
In case paging is not supported for the guest, currently meaning everything except x86,
both values are the same and contain the physical address of the segment.

\subsection{Java's 2GB limit}\label{sec:Java_2GB}
With Java being a product of 1995, it was developed when 32-bit computers were still the standard.
Because of this, most interfaces in Java work with 32-bit signed integers instead of 64-bit integers.
In addition, Java has no direct support for unsigned integers.
This design also affects arrays, which thanks to this can only hold a maximum of $2^{31}$ elements,
which for a byte array results in 2GiB of maximum available space in such an array.
Since this thesis should also work on QEMU instances running on more than 2GiB of RAM,
which would consequently produce an ELF file larger than the Java limit,
this issue must be circumvented.
When storing longs in an array, this can theoretically be increased to $2^{34}$, or 16GiB of data.
For doing something like this, it however also needs to be mentioned that one cannot simply convert a long into 8 bytes in Java by simple casting,
but instead shift operations are required, which hurt performance.
To circumvent this, 2 approaches were implemented:

\paragraph{Input Streams}
Input streams are a fundamental interface in the Java world,
and they supply an undefined amount of bytes to any given consumer.
The \enquote{undefined} here explicitly allows for an amount of bytes higher than the usual $2^{31}$ byte limit.
Since Java 9, this interface was explicitly expanded with the goal to at least somewhat allow 64-bit longs\footnote{See https://docs.oracle.com/en/java/javase/22/docs/api/java.base/java/io/InputStream.html#transferTo(java.io.OutputStream)}.
This is a decent substitute for transferring bytes using an array,
and while not every piece of code expects it and produces undefined behavior
once over $2^{31}$ bytes were received, at least for the libraries in this thesis that was not an issue.

\paragraph{BigArrays}
The Java library \emph{fastutil}\footnote{See https://fastutil.di.unimi.it/} offers so-called \enquote{Big Arrays},
which are 2-dimensional arrays with helper methods allowing them to be used like normal arrays.
This increases the maximum number of bytes stored in \enquote{an array} to $2^{62}$,
which should suffice for the purposes of this thesis.
\emph{fastutil} does not provide an Input Stream for such an array itself,
however developing one was a rather trivial exercise.

\subsection{Checkpoint format}
Each memory region holds 5 values when stored as a JSON object:
\begin{itemize}
    \item \emph{size} stands for the absolute size in bytes reported by \emph{filesz}.
    \item \emph{startPhysicalAddress} stores the first address this region holds when mapped to a QEMU instance.
    \item \emph{startVirtualAddress} stores the first address this region holds accessible from the guest operating system.
    If paging is not supported on the current architecture, its content is identical to \emph{startPhysicalAddress}.
    \item \emph{hash} stores the hash computed for this memory segment as Base64.
    \item \emph{storageLocation} holds the absolute path this memory region's dump is stored at on disk.
\end{itemize}

\sectionblock devicess}
This section again was rather trivial, even though it requires more computation than the CPUs.
The QMP commands foblock deviceses seem to be the most capable of the data points discussed here,
likely because attaching and removiblock devicesces during runtime is a task relevant not only to the scientific community.\todonote{Is that too on the nose?}
The command \emph{query-block} provides all necessary information for an extraction process,
and though one may feel overwhelmed when first seeing its output,
which in parts is because it includes a lot of deprecated and invalid data,
it is very easy to extract which devices are connected, whether they contain media and where said media is located.

The extraction process therefore only consists of the stepts
\begin{enumerate}
    \item Get the arrayblock devicesices from QEMU.
    \item Parse all their names and internal identifiers.
    \item Check if they have media inserted, and if so, get the image and compute its hash.
    \item If it's not a duplicate, copy the image to the specified checkpoint directory.
\end{enumerate}

\subsection{Checkpoint format}
For each blockdevice, the following fields get stored in JSON:
\begin{itemize}
    \item \emph{deviceName} is the human-readable name given to this device.
    \item \emph{qdevID} is the internal name used in QEMU, comparable to /dev/sdX in Linux.
    This may not always exist in QEMU, and may be the QOM path if no ID was set.
    \item \emph{virtualSize} is the capacity of the device shown to the guest operating system.
    If no media is inserted, this shows the value zero,
    however a value of zero is no indication about the absence of any media.
    \item \emph{actualSize} is the actual size of the image on disk.
    If no media is inserted, this has the value zero,
    however a value of zero is no indication about the absence of any media.
    \item \emph{hasMedia} shows whether there is actually an image mounted onto this device.
    If media is inserted, these additional fields are available:
    \begin{itemize}
        \item \emph{originalPath} holds the location where the image was originally stored at.
        \item \emph{hash} stores the hash computed for the media as Base64.
        \item \emph{storageLocation} holds the absolute path of the copy of this blockdevice's image.
    \end{itemize}
\end{itemize}

\section{External devices}
There currently is no generic method to interact with devices attached to a QEMU instance
which do not fall under one of the three aforementioned categories.
There are dedicated commands for trusted platform modules, input devices, character devices\footnote{In QEMU, keyboards are sometimes character devices and other times input devices, making treating them in a unified way especially difficult.},
audio and video cards, generic PCI cards and many more\cite{qmp-commands}.
Implementing these commands could likely be done to allow for some kind of checkpointing,
and many do provide some way of gathering memory information,
however one ultimately collides with the issues already mentioned in \autoref{sec:JNA_failure},
especially since I have not yet discovered a dump command for any device group.

The dump commands discussed in \autoref{dump_memory} and \autoref{sec:memsave}
have no option to switch them to different memory regions, but only operate on main memory.
As can be seen in \autoref{fig:mem_ARM_full} and \autoref{fig:xemu_full},
the memory addresses overlap, meaning it is also not possible to access these regions
by cleverly choosing addresses.
This means one would have to individually research each kind of device to dump its data,
while in such a process also making sure special kinds of data not directly stored in the device's memory,
like CPU registers, are included as well.
For these reasons, checkpointing these devices was not pursued further in this thesis.