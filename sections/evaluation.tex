%% LaTeX2e class for student theses
%% sections/evaluation.tex
%%
%% Karlsruhe Institute of Technology
%% Institute of Information Security and Dependability
%% Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.5, 2024-02-12

\chapter{Evaluation}\label{chap:Evaluation}
Evaluating the work of this thesis was no trivial task.
The most important part of an evaluation is obviously the correctness of the gathered results,
if the results of any program are not correct, there is no point in analyzing its performance or flexibility.

A very interesting approach here would of course have been to use the tool developed by \citeauthor{kitcheckpoints}
and to compare the results of each solution.
This especially would have offered the possibility to compare two very different approaches,
as \citeauthor{kitcheckpoints} directly modified QEMU's code to achieve a goal similar to the one of this thesis,
while this thesis aims to do so without manipulating QEMU in any way to achieve better compatibility.
Such a comparison would not only have been interesting regarding the correctness of either solution,
but especially in regards to performance and resource usage,
as one might suspect \citeauthor{kitcheckpoints} solution to be better in these regards,
as my solution uses Java and JSON, both a lot slower tools when compared to C and SQL which were used by \citeauthor{kitcheckpoints}.
Unfortunately, I could not find the original code \citeauthor{kitcheckpoints} wrote for his paper,
as they did not link any kind of such a repository in his paper.
Additionally, one may question whether it would even be possible to get his code to work 11 years after publication,
as QEMU has of course changed massively in the meantime, which may have broken the modifications they made to QEMU.
Because of this, it was not possible to perform such a direct comparison.
It was also refrained from using his original numbers,
as I do not have similar hardware available,
making any kind of performance comparison obsolete,
especially when considering the technological advancements of the last decade\cite{kitcheckpoints}.

Another way to verify any extracted data would have been to re-inject them into a running QEMU instance and see if the result is valid.
However, developing such a mechanism is not the direct goal of this thesis,
and when looking at the commands QMP and QHM offer, it is also evident such a task is not the purpose of these APIs.
This means, such a reinjection could likely be its own paper,
and would thereby throw this chapter completely out of proportion
and make it a thesis inside a thesis.
\eqnuote{Re-Injection} is therefore reserved as a topic for future work in \todonote{Link future work}
and will not be discussed further for the purposes of evaluating this thesis.

On the point of specifications, the system used for any kind of testing consists of the following parts:
\begin{itemize}
    \item CPU: AMD Ryzen 9 7900X with overclocked all-core boost of 5.5GHz
    \item RAM: 48GiB DDR5-7000 CL36
    \item GPU\footnote{None of the tests require a GPU, only included for completeness}: Nvidia Geforce RTX 3090 overclocked to 2.1GHz
    \item Hard Drives:
    \begin{itemize}
        \item Samsung SSD 990 Pro 1TB
        \item Samsung SSD 970 Evo Plus 1TB
        \item 2* Western Digital Blue 1TB in Raid 0 cached by the 990Pro
    \end{itemize}
    \item Operating System: Windows 11 Pro
\end{itemize}
This is not a clean Windows install, but instead many tools a developer may use,
like IntelliJ, VS Code, Mircosoft Office and so on are running on this machine.

\section{Correctness \& Completeness}
As such, the first thing to discuss is whether the results produced actually hold up.
To achieve this, the results are discussed individually for each region in \autoref{chap:implementation}.
The main issue of this section is the lack of any alternative software to gather the same data for comparison.
While there are tools like QEMU's internal checkpointing mechanism,
to compare the data gathered one would first need to parse QEMU's results and the results of this paper,
which brings multiple possibilities for errors not directly related to this thesis itself,
especially as QEMU's checkpoints are stored in a serialized format,
while this thesis aims to use a general purpose format, namely JSON.
To work around this, for each kind of data point,
solutions were found and any compromises made are mentioned and justified.

\subsection{CPU}\label{sec:eval_cpu}
When developing the extraction for the CPU,
the main problem was to work around all the oddities described in \autoref{sec:info_registers}.
The data itself was however externally accessible in a human-readable form,
which means, under the assumption that QEMU provides correct data through this API,
the correctness of the data only depends on whether the parser is implemented correctly.
To verify the extracted data, one must now somehow compare the output of the \emph{info registers} command
with the data present in the internal model of the developed solution.

To automate such a process, one would have to implement another parser that parses \emph{info registers}
and then compare the output of both parsers and check whether they are correct.
This approach is rather ridiculous, as it assumes when writing the same program 2 times,
one of them must be correct.

Therefore, to not write the same code with the same errors twice,
this was done manually for ARM, x86 and x64 during development,
and for these architectures the parsing now works correctly.
There are of course different architectures which contain different data
and may format the data differently, however the basic structure is always similar,
meaning the developed solution always extracts its data correctly.

Still, when operating on other architectures, for example RISC-V or Power-PC,
special care should be attributed to verifying
the results produced by this thesis' developments
correctly handle such architectures, especially in the beginning to avoid exhaustive reworks later on.

\subsubsection*{Completeness}
While the data extracted is correct, the bigger question here is whether all data is extracted.
On ARM, for example, there may exist the line \enquote{FPU disabled}.
Such plaintext is not parsed at all,
as there is no easy resource containing all such special data points one may encounter,
and therefore they are not implemented.
Additionally, the tool only parses a single flag array reported by \emph{info registers} per CPU,
meaning any flags not contained in the first registers do not get included in a checkpoint.
On x86, this means the registers associated with the pointer registers,
containing information about the access restrictions of the referenced memory region,
are currently not included at all in a checkpoint.

If one intends to use the checkpoints produced by this thesis for their own purposes,
it is therefore imperative they check whether the data included in a checkpoint is sufficient for their work.
Especially analyzing the flags of any CPU requires special care
and perhaps additional modifications of the code produced by this thesis.

\subsection{System memory}
The biggest part of the implementation phase of this thesis
was the extraction of data from the guest's memory.
This also warrants special attention in the evaluation of this data,
as all of the approaches attempted faced some issues during implementation.

In addition, this the only one of the three cases
in which the data provided by QEMU cannot be used as-is.
Instead, a third-party format is used between QEMU and the extraction,
which is implemented using another library
which is not maintained by a big organization
and does not seem to be used in a widespread manner.

At the same time, the amount of data to check for consistency is rather large,
multiple gigabytes in most cases.
Like in \autoref{sec:eval_cpu}, this makes the easiest way of verifying the correctness of the data
to directly compare it with the output of QEMU.
For this, the QHM commands discussed in \autoref{sec:memsave} seem like good candidates.
Since the tool developed in this thesis produces multiple dump files
in relation to the memory regions included in the original ELF file,
those commands can be used to extract the exact same regions
and then compare the contents of these files.
The code of this test is shown in \autoref{fig:test_memory}

\begin{figure}[h]
    \begin{ffcode}
        QMPInterface inter = new QMPInterface("localhost", 4444);
        inter.executeCommand(Stop.INSTANCE);
        ELFDump elf = new ELFDump(inter);
        inter.executeCommand(elf);
        elf.awaitCompletion();
        for (MemorySegment segment : elf.getSegments()) {
            long startAddress = segment.getStartPhysicalAddress();
            long size = segment.getSize();
            Path target = Paths.get("D:\\Lars\\out\\" + startAddress + ".dmp");
            PMemsave save = new PMemsave(startAddress, size, target);
            inter.executeCommand(save);
            InputStream elfInput = segment.getInputStream();
            try (InputStream dumpInput = Files.newInputStream(target)) {
                int elfRead;
                do {
                    elfRead = elfInput.read();
                    int dumpRead = dumpInput.read();
                    if (elfRead != dumpRead) System.out.print("Found mismatch");
                } while (elfRead != -1);
            }
        inter.executeCommand(Continue.INSTANCE);
  \end{ffcode}
  \caption{The code used to check for correct extraction of memory contents.}
  \label{fig:test_memory}
\end{figure}

As can be seen, this code writes \enquote{Found mismatch} to the standard output
if any discrepancies between the two files were found.
This test did in fact unearth some issues with the code,
mainly related to the implementation of an Input Stream to read from a 2D array
as discussed in \autoref{sec:Java_2GB},
but in the end, all of the data was consistent.

\subsubsection*{Inconsistency during the development cycle}
At one point during early testing,
it was observed most of the memory dumps started with the same group of bytes.
At that time however, no bigger focus was placed on this curiosity,
as there were still other issues waiting to be solved.
Later on, this behavior could not be replicated anymore,
even though no explicit effort was taken before to address it.
It is possible this \enquote{bug} was solved by accident
or while fixing another part of the software,
however such oddities should be mentioned here to make others aware of them and look out for them.

\subsubsection*{Deduplication}
The tool offers the ability to deduplicate checkpoint data,
however it is nowhere near as sophisticated as the solution offered by \citeauthor{kitcheckpoints}.
Currently the tool cannot load comparison data from disk,
Although such a process would be trivial to implement,
as only the created JSON needs to be parsed.
Still, this results in the tool only deduplicating checkpoint data
against checkpoints created previously by the same process.
This can be observed by the amount of memory files created
when creating follow-up checkpoints.
The more time has passed between 2 checkpoints,
the more memory dump files get stored in the checkpoint directory.
In addition, when the emulator is paused and does not run between 2 checkpoints, no new memory files get created.

\subsection{Block devices}
As the implementation of block devices was again rather trivial,
there is no explicit task to test here.
The tool simply copies the files from the original location provided by QEMU
to the destination desired by the user.
And, if we assume the copy process provided by Java works,
it can be assumed this step is fully functional.
There may be errors in regards to permissions of the file to copy,
however that is not something this thesis has to look at.

\subsubsection*{Deduplication}
The approach used for deduplicating block devices is essentially the same as the one for memory dumps.
The code is nearly identical, as the comparison of hashes for both types is the same.
This also allows for the assumption, that if the deduplication works for memory regions,
it also works for block devices.
The main difference is the rate of follow-up checkpoints is much lower,
block devices generally experience fewer changes than memory regions.
Depending on the use case, this may differ however as block devices are copied as a whole and not in parts like main memory.

\section{Performance}\label{sec:eval_performance}
Given one of the goals of this thesis is to allow for performance improvements,
such performance should be tested.
As discussed in the beginning of \autoref{chap:Evaluation},
the approach used by this thesis is not the most performance-oriented,
but instead the main goal is to allow for flexibility.
This will likely mean a performance disadvantage when compared to the solution offered by \citeauthor{kitcheckpoints},
but such a comparison is, as mentioned, not possible anyway.

An additional issue here are the problems \citeauthor{Java_Benchmarking} discussed in \citetitle{Java_Benchmarking}\cite{Java_Benchmarking}.
Any short testing of the performance of a Java program, such as the one developed by this thesis,
will likely produce results that are hard to validate on other machines,
as the behavior of the just-in-time compiler makes this somewhat unpredictable.
The general assumption is Java programs generally reach a \enquote{steady state} after a certain \enquote{warm-up phase}.
However, as \citeauthor{Java_Benchmarking} described, the duration of this \enquote{warm-up phase}
is usually just rough guesswork by developers,
and whether any such guess made is educated is often unpredictable\cite{Java_Benchmarking}.

To evaluate the performance of the program,
only the performance for creating two checkpoints,
an initial one and the first follow-up,
is measured, and it is then assumed the performance of the program will increase as Java's optimizations kick in.
This effectively makes the results gathered in this thesis a \enquote{Worst-case}.
To do this, the time taken to create a checkpoint will be measured by Java itself
by using the \emph{System.currentTimeMillis()} method\footnote{See \href{https://docs.oracle.com/en/java/javase/22/docs/api/java.base/java/lang/System.html#currentTimeMillis()}{https://docs.oracle.com/en/java/javase/22/docs/api/java.base/java/lang/System.html}.}
and calculating the difference between before and after the creation of a checkpoint.
The experiment uses a QEMU instance displaying the \enquote{Select Language} screen of the Windows 11 Setup,
running with 4 CPU cores and 2GiB of RAM.
The QEMU command to set up this device is shown in \autoref{fig:QEMU_Windows_Test}.

\begin{figure}[h]
    \begin{ffcode}
        qemu-system-x86_64 -m 1800 -boot d -cdrom "G:\Win11_23H2_German_x64v2.iso" -smp 4 -qmp tcp:localhost:4444,server,wait=off
    \end{ffcode}
    \label{fig:QEMU_Windows_Test}
    \caption{The command used for the creation of the QEMU test instance}
\end{figure}

In this command, the first issue can already be seen.
A normal Windows 11 install requires at least 4GiB of system memory.
However, when trying to checkpoint such an instance, the process crashed with an \emph{OutOfMemory}-Exception,
because the jelf library\cite{jelf} attempted to create a byte buffer with more than 2GiB of space.
This means jelf is not able to handle such large files because of the limitations discussed in \autoref{sec:Java_2GB}.

The exact results for this benchmark are presented in \autoref{tab:benchmark}.
From those, an average duration of $15.489$ respective $13.420s$ for the creation of an initial and a follow-up checkpoint can be calculated.
The median is $15.094s$ respective $13.577s$.
During testing, it was discovered the main slowdown is the copying of the 6.6GiB ISO file containing the Windows Installer.
When the ISO is stored on an external USB drive, the time for the initial checkpoint tripled.
This lends credibility to the assumption that the main performance bottleneck is the hard drive speed,
which makes sense regards to the compromises from \autoref{sec:memory_extraction}.
Since memory dumps require disk access because of QEMU's limitations,
even on NVMe drives like in this benchmark, the drive accesses are the main bottleneck during an extraction process.

To further illustrate this, the total data to process is around 8.4GiB. 6.6GiB for the Windows ISO and 1.8GiB of memory.
When calculating this for the initial creation, this gives a \eqnuote{speed} of $\frac{6.6GiB + 1.8 GiB}{15s}=0.56\frac{GiB}{second}$,
which is a third of what the NVMe drives installed in my system achieve during everyday operations.
And since 3 acceses\footnote{QEMU dump to disk, JELF read from disk, Java write to disk} are required during operation,
this number seems to be reasonable.
This is also in line with the slight speed-up for secondary checkpoints, as the third instance of writing usually gets smaller.

\paragraph{Resource usage} The other important part to consider here is the resource usage exhibited by the tool.
As demonstrated by \citetitle{Java_Performance},
Java has a very high memory footprint\cite{Java_Performance}.
Additionally, the QEMU documentation itself mentions the command \emph{dump-guest-memory}
may allocate several gigabytes of RAM when paging is used,
which this tool does if available.
QEMU does not explicitly specify when this occurs,
only that it \enquote{can happen for a large guest, or a malicious guest pretending to be large.}\cite{qmp-commands}.
They do not explicitly specify what is considered a \enquote{large guest}.
As Java however loads the whole dump produced by QEMU into memory effectively twice,
once for Jelf to parse and afterwards to write the stripped files to disk,
the memory footprint of this tool is rather catastrophic,
as the memory usage of any guest is tripled at least.

A computing speed of around $560\frac{MiB}{second}$ is a very good result,
and for larger files this speed will likely go up.
As with standard hard drives, higher speeds are very hard to achieve,
especially when using parallel accesses like this tool does.
For this reason, it can be concluded the performance itself displayed by this tool is very good,
however the resource usage during the process needs to be reduced,
as currently more than triple the resources are required to run the tool.

\section{Flexibility}\label{sec:eval_flexibility}
The main goal of this thesis and also its main differentiation from \citeauthor{kitcheckpoints}
was to make this solution as flexible as possible.
The main goal was to only use APIs QEMU provides
and to not modify the code of QEMU in any way.

To achieve this goal, compromises needed to be made.
As mentioned in \autoref{sec:external_approach}, external devices are in no way supported because the support QEMU offers for this is too restrictive.
Similarly, the restrictions in place for extracting CPU data mentioned in \autoref{sec:info_registers} mean for any CPU architecture there may be data this tool does not expect.
Both these obstacles mean the result of this thesis needs to be used with care
and while it may be useful in some cases, in other cases manual intervention and additional code is required to make it work.
This is a clear case of future work, to further integrate target-dependent specialties.

One massive problem is however the 2GiB limit imposed by Java discussed in \autoref{sec:Java_2GB}.
Because of this, currently this tool cannot be used on any emulation instance using more than 2GiB of main memory.
In the world of IoT devices, this may not be a dealbreaker, but since even handheld devices offer multiple Gigabytes of RAM,
this thesis cannot be applied to many everyday devices.
The way to solve this problem would be to create a library similar to jelf\cite{jelf},
however with the capability of processing files larger than 2GiB.
The development of such a library could potentially be worth its own paper,
and is therefore a case for future work as well,
even though it can be classified as a \emph{neccesity} for the usage of this thesis.

Another solution for this problem would be to use another programming language or environment to implement the tool.
Java was explicitly chosen since a Java program can basically run anywhere QEMU runs as well,
therefore supposedly offering a great range of flexibility.
Java's main competitor, C#, suffers from the same problem however.
While it does allow for the usage of unsigned integers,
this still only allows for a maximum array size of 4GiB.
Considering the minimum requirement for Windows 11 is 4GiB,
which is not a pleasant experience,
even such a limit would likely run into the same issues as the ones in this thesis.
Generally, such large objects in memory are often reserved for low-level languages like C or C++.
These languages may therefore be a viable candidate for solving this problem,
however they make the program less flexible in other ways,
especially when the program should be used on different host operating systems.

The tool is however very flexible in deciding which parts of the accessible data are to be gathered.
While the basic CLI only creates checkpoints and the rules for such creation are rather limited,
the tool can also be used as a library, using which the user can individually select which data points they need
and could theoretically even collect different data points at different intervals.
With the existence of the interfaces \emph{Command} and \emph{QHMCommand},
it is also very easy to create new commands which gather data points this thesis does not cover.

\section{Usability}
As this tool is intended for other developers,
there were no special requirements for its usability or ease of use.
The tool basically resembles a library for creating checkpoints,
with a basic CLI option allowing for the creation of full checkpoints,
but using it as a library provides much more abilities.
The CLI does document itself using the \ff{-h} flag,
however there is no GUI which would make the experience easier for inexperienced users.
However such inexperienced users likely also have no use for the data produced by this thesis.

\section{Conclusion}
While this thesis does provide useful results,
it is clear more research is needed if the approach offered by this thesis is to be pursued further.
Especially the restrictions outlined in \autoref{sec:eval_flexibility}
mean any time the tool developed by this thesis is to be applied,
it must first be thoroughly checked whether the tool can correctly handle all the data required.
This makes this thesis a good basis for further research into the topic of extracting data from an emulation,
but the results are not yet complete.